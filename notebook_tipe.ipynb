{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie exrtaction des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xlrd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# import\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxlrd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xlrd'"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random as rd\n",
    "import xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import bibliothèque traitement des fichiers tableur + ouverture du document\n",
    "doc = xlrd.open_workbook('forestfires.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction première (et seule) feuille du fichier + ses dimensions\n",
    "feuille = doc.sheet_by_index(\n",
    "lignes, colonnes = feuille.nrows, feuille.ncols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction noms des caractéristiques\n",
    "feature_names = []\n",
    "for i in range(colonnes - 1): # -1 pour enlever la dernière colonne (résuultats)\n",
    "    # on prend pas les coordonnées (colonnes 0 et 1)\n",
    "    if i >= 2:\n",
    "        feature_names.append(feuille.cell_value(rowx = 0, colx = i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertir les mois et les jours de chaine en entiers\n",
    "month = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "day = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n",
    "\n",
    "def month_2_int(i):\n",
    "    n = len(month)\n",
    "    for k in range(n):\n",
    "        if feuille.cell_value(rowx = i, colox = 2) == month[k]:\n",
    "            return k\n",
    "\n",
    "def day_2_int(i):\n",
    "    n = len(day)\n",
    "    for k in range(n):\n",
    "        if feuille.cell_value(rowx = i, colox = 3) == day[k]:\n",
    "            return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction données dans tableau numpy (scikit avec type Bunch)\n",
    "data_list = []\n",
    "for i in range(1, lignes):\n",
    "    elem = []\n",
    "    for j in range(colonnes - 1):\n",
    "        # omission volontaire des colonnes de coordonnées 0 et 1\n",
    "        if j>= 4:\n",
    "            elem.append(float(feuille.cell_value(rowx = i, colx = j)))\n",
    "        elif j == 2: # mois transformés\n",
    "            elem.append(month_2_int(i))\n",
    "        elif j == 3: # jours transformés\n",
    "            elem.append(day_2_int(i))\n",
    "    data_list.append(elem)\n",
    "    elem = []\n",
    "\n",
    "data = np.array(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mettre les targets dans un tableau numpy de 1 colonne target\n",
    "target_temp = []\n",
    "for i in range(1, lignes):\n",
    "    target_temp.append(float(feuille.cell_value(rowx=i, colx=12)))\n",
    "\n",
    "target = np.array([np.log(1+X) for X in target_temp]) # transfo logarithmique\n",
    "# rapprocher valeurs des abscisses (sans ça: imprécisions pour grandes valeurs)\n",
    "\n",
    "fire= [data, feature_names, target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fabrication d'un second set pour un second modèle sur de petits incendies\n",
    "# calcul proportion d'échantillons de target < = 15 hectares\n",
    "n = len(target)\n",
    "s = 0\n",
    "for i in range(n):\n",
    "    if target_temp[i] <= 15:\n",
    "        s += 1\n",
    "\n",
    "# qui représentent +85% des échantillons\n",
    "print(s/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = []\n",
    "targ = []\n",
    "for i in range(n):\n",
    "    if target_temp[i] < 15:\n",
    "        targ.append(target[i])\n",
    "        dat.append(data[i])\n",
    "targ = np.array([np.log(1+X) for X in targ])\n",
    "dat = np.array(dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sur la nécessité d'une transformation logarithmique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(target_temp).plot.hist(grid=True, bins=40, rwidth=.9, color=\"#000000\")\n",
    "plt.title('Allure de la distribution des surfaces')\n",
    "plt.xlabel('Surface brûlée')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = pd.Series(target).plot.hist(grid=True, bins=40, rwidth=.9, color=\"#000000\")\n",
    "plt.title('Allure de la distribution des surfaces')\n",
    "plt.xlabel('Surface brûlée')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.grid(axis='y', alpha=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie division du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dans le module model_selection, train_test_split pour diviser les data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# séparation en 2 jeux (un de train et un de test)\n",
    "data_split = train_test_split(data, target, train_size=0.75, test_size =0.25, random_state =70, shuffle=False)\n",
    "data_train, data_test, target_train, target_test = data_split\n",
    "\n",
    "# une seconde fois 2 jeux (cette fois-ci target <= 5 hectares uniquement)\n",
    "dat_spl = train_test_split(dat, targ, train_size =.75, test_size=.25, random_state=70, shuffle=False)\n",
    "dat_tra, dat_tst, targ_tra, targ_tst = dat_spl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie mise à l'échelle des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m scalerS \u001b[39m=\u001b[39m StandardScaler() \u001b[39m# pour toutes les données\u001b[39;00m\n\u001b[1;32m      4\u001b[0m scalerS2 \u001b[39m=\u001b[39m StandardScaler() \u001b[39m# les données ayant target <= 15 ha\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m scalerS\u001b[39m.\u001b[39mfit(data_train), scalerS2\u001b[39m.\u001b[39mfit(dat_tra)\n\u001b[1;32m      8\u001b[0m data_train_tr \u001b[39m=\u001b[39m scalerS\u001b[39m.\u001b[39mtransform(data_train)\n\u001b[1;32m      9\u001b[0m data_test_tr \u001b[39m=\u001b[39m scalerS\u001b[39m.\u001b[39mtransform(data_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scalerS = StandardScaler() # pour toutes les données\n",
    "scalerS2 = StandardScaler() # les données ayant target <= 15 ha\n",
    "\n",
    "scalerS.fit(data_train), scalerS2.fit(dat_tra)\n",
    "\n",
    "data_train_tr = scalerS.transform(data_train)\n",
    "data_test_tr = scalerS.transform(data_test)\n",
    "dat_tra_tr, dat_tst_tr = scalerS2.transform(dat_tra), scalerS2.transform(dat_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie calcul du score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(vrais_resultats, predictions, eps=10):\n",
    "    # renvoie RMSE et % de points tels que abs(prediction-résultat) <= eps\n",
    "    ecart = np.abs(vrais_resultats - predictions).ravel()\n",
    "    # ravel pour aplatir un éventuel tableau de plusieurs dimensions\n",
    "    RMSE, s, n = 0, 0, len(ecart)\n",
    "    for i in range(n):\n",
    "        RMSE += ecart[i]*ecart[i]\n",
    "        if ecart[i] <= eps:\n",
    "            s += 1\n",
    "    return np.sqrt(RMSE/n), 100*(round(100*s/n)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Méthode de recherche heuristique de C (ici pour le modèle 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "eps = 1e-1\n",
    "\n",
    "Y3, Y4 = [], []\n",
    "CC = [i/10 for i in range(1, 10)] + [i for i in range(1, 11)]\n",
    "\n",
    "for C in CC:\n",
    "    model2 = SVR(C=C, gamma='auto', kernel='rbf', epsilon=eps)\n",
    "    model2.fit(dat_tra_tr, targ_tra)\n",
    "    log_predictions2 = model2.predict(dat_tst_tr)\n",
    "    predictions2 = [np.exp(X)-1 for X in log_predictions2]\n",
    "    rms, per = score(targ_tst, predictions2, eps=1)\n",
    "    Y3.append(round(100*rms)/100), Y4.append(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_graph = plt.scatter(CC,Y3, color='blue', marker='o')\n",
    "plt.grid()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Comparaison de l'erreur moyenne quadratique du modèle 2 en fonction de C\")\n",
    "plt.legend([rms_graph], ['rms'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_graph = plt.scatter(CC,Y4, color='orange')\n",
    "plt.grid()\n",
    "plt.xlabel(\"C\")\n",
    "plt.ylabel(\"efficacité à 1 hectare près\")\n",
    "plt.title(\"Comparaison de l'efficacité du modèle 2 en fonction de C\")\n",
    "plt.legend([per_graph], ['per ha'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie initialisation de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "C = 0.2\n",
    "eps = 1e-1\n",
    "\n",
    "model = SVR(C=C, gamma='auto', kernel='rbf', epsilon= eps)\n",
    "model.fit(data_train_tr, target_train)\n",
    "model2 = SVR(C=C, gamma='auto', kernel='rbf', epsilon=eps)\n",
    "model2.fit(dat_tra_tr, targ_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce qu'on peut retrouver à propos du modèle (ici model) après l'apprentissage\n",
    "# attributs appelables: support_ , support_vectors_ , dual_coef_ , intercept_\n",
    "\n",
    "n_sv = len(model.support_)\n",
    "k = rd.randint(0, n_sv - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('- Le {}ème vecteur de support est le vecteur numéro {} du jeu de départ.'.format(k, model.support_[k]))\n",
    "\n",
    "sv_k = model.support_vectors_[k]\n",
    "print(\"- Les {} coordonnées de ce vecteur dans l'espace de départ sont \\n {} \".format(len(sv_k), sv_k))\n",
    "\n",
    "print('- Son coefficient dual est {}'.format(model.dual_coef_[0, k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie prédiction des modèles (tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prédictions model (encore sous forme logarithmique)\n",
    "log_predictions = model.predict(data_test_tr)\n",
    "predictions = [np.exp(X)-1 for X in log_predictions]\n",
    "\n",
    "# prédictions model2 (encore sous forme logarithmique)\n",
    "log_predictions2 = model2.predict(dat_tst_tr)\n",
    "predictions2 = [np.exp(X)-1 for X in log_predictions2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Représentation logarithmique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y1, Y2 = [], [], []\n",
    "for i in range(1,8):\n",
    "    Y1.append(score(target_test, predictions, eps=i)[1])\n",
    "    Y2.append(score(targ_tst, predictions2, eps=i)[1])\n",
    "    X.append(i)\n",
    "    \n",
    "\n",
    "mod1 = plt.scatter(X,Y1, color='red', marker='o')\n",
    "mod2 = plt.scatter(X,Y2, color='green', marker='*')\n",
    "plt.grid()\n",
    "plt.xlabel(\"hectares d'erreur acceptable\")\n",
    "plt.ylabel(\"efficacité en %\")\n",
    "plt.title(\"Comparaison de l'efficacité des 2 modèles\")\n",
    "plt.legend([mod1,mod2], ['modèle complet', 'modèle 15 ha'], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(n=(1,5))\n",
    "def eps_change(n=2):\n",
    "    global rms, per\n",
    "    rms, per= score(target_test, predictions, eps=n)\n",
    "    rms = round(abs(rms)*1e5)/1e5\n",
    "    print('Toutes les données: {} (RMSE), {}% (pourcentage de réussite)'.format(rms, per))\n",
    "    rms, per = score(targ_tst, predictions2, eps=n)\n",
    "    rms = round(abs(rms)*1e5)/1e5\n",
    "    print('Les données pour target < 15 ha: {} (RMSE), {}% (pourcentage de réussite)'.format(rms, per))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partie visualisation de la précision des prédictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visu_diff_abs(pred, target):\n",
    "    # visualiser la différence absolue entre deux tableaux\n",
    "    tst = len(target)\n",
    "    return [round(abs(target[k]-pred[k])*100)/100 for k in range(tst)]\n",
    "\n",
    "def visu_diff_rel(pred, target):\n",
    "    # visualiser la différence relative entre 2 tableaux\n",
    "    tst = len(target)\n",
    "    L = []\n",
    "    for i in range(tst):\n",
    "        if target[i] == 0:\n",
    "            L.append((-1)*round(abs(target[i]-pred[i])*100)/100)\n",
    "            continue\n",
    "        L.append(round(abs((target[i]-pred[i])/target[i])*100)/100)\n",
    "    \n",
    "    return L\n",
    "\n",
    "diff_abs = visu_diff_abs(predictions, target_test)\n",
    "diff_abs2 = visu_diff_abs(predictions2, targ_tst)\n",
    "diff_rel = visu_diff_rel(predictions, target_test)\n",
    "diff_rel2 = visu_diff_rel(predictions2, targ_tst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(diff_abs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(diff_abs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(diff_rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(diff_rel2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcul R^2 second modèle (sur les données d'entraînement)\n",
    "\n",
    "p = [np.exp(X)-1 for X in model2.predict(dat_tra_tr)]\n",
    "n = len(model2.support_) # nb de SV\n",
    "u, v = 0, 0\n",
    "for i in range(n):\n",
    "    v += (targ_tra[i]-p[i])**2\n",
    "    if not i in model2.support_:\n",
    "        u += (targ_tra[i]-p[i])**2\n",
    "\n",
    "print(1 - u/v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# même calcul pour le premier modèle\n",
    "\n",
    "p = [np.exp(X)-1 for X in model.predict(dat_tra_tr)]\n",
    "n = len(model.support_) # nb de SV\n",
    "u, v = 0, 0\n",
    "for i in range(n):\n",
    "    v += (targ_tra[i]-p[i])**2\n",
    "    if not i in model.support_:\n",
    "        u += (targ_tra[i]-p[i])**2\n",
    "\n",
    "print(1 - u/v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tp_mnist_conda",
   "language": "python",
   "name": "tp_mnist_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
